{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "IMDB_train = pd.read_csv('./IMDB-train.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_train_y = IMDB_train[:][1]\n",
    "IMDB_valid = pd.read_csv('./IMDB-valid.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_valid_y = IMDB_valid[:][1]\n",
    "IMDB_test = pd.read_csv('./IMDB-test.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_test_y = IMDB_test[:][1]\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [IMDB_train, IMDB_valid]\n",
    "frames_y = [IMDB_train_y, IMDB_valid_y]\n",
    "IMDB_train = pd.concat(frames)\n",
    "IMDB_train_y = pd.concat(frames_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    new_data = []\n",
    "    #i = 0\n",
    "    for sentence in (data[:][0]):\n",
    "        #clean = re.compile('<.*?>')\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rm_numbers(data):\n",
    "    new_data = []\n",
    "    #i = 0\n",
    "    for sentence in (data):\n",
    "        new_sentence = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", sentence)\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMDB_train = preprocessing(IMDB_train)\n",
    "IMDB_test = preprocessing(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMDB_train_rm_num = rm_numbers(IMDB_train)\n",
    "IMDB_test_rm_num = rm_numbers(IMDB_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of n-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words='english', max_features =30000)\n",
    "bigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(2, 2), stop_words='english', max_features =30000)\n",
    "trigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(3, 3), stop_words='english', max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unigram = unigram.fit_transform(IMDB_train).toarray()\n",
    "test_unigram = unigram.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigram = bigram.fit_transform(IMDB_train).toarray()\n",
    "test_bigram = bigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trigram = trigram.fit_transform(IMDB_train).toarray()\n",
    "test_trigram = trigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words=None, max_features =30000)\n",
    "bigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(2, 2), stop_words=None, max_features =30000)\n",
    "trigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(3, 3), stop_words=None, max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram_w_sw = unigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_unigram_w_sw = unigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigram_w_sw = bigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_bigram_w_sw = bigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trigram_w_sw = trigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_trigram_w_sw = trigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unibigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 2), stop_words=None, max_features =30000)\n",
    "allgram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 3), stop_words=None, max_features =30000)\n",
    "allgram_wo_lm = CountVectorizer(analyzer='word', ngram_range=(1, 3), stop_words=None, max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unibigram_w_sw = unibigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_unibigram_w_sw = unibigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_allgram_w_sw = allgram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_allgram_w_sw = allgram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_allgram_wo_lm = allgram_wo_lm.fit_transform(IMDB_train).toarray()\n",
    "test_allgram_wo_lm = allgram_wo_lm.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_allgram_rm_num = allgram_w_sw.fit_transform(IMDB_train_rm_num).toarray()\n",
    "test_allgram_rm_num = allgram_w_sw.transform(IMDB_test_rm_num).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Naive_Bayes_B(train_data, train_label, valid_data, valid_label, test_data, test_label):\n",
    "    tuned_parameters = [{'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]}]\n",
    "    test_valid_fold = np.r_[ np.full(train_label.shape[0], -1),np.ones(valid_label.shape[0])]\n",
    "    ps = PredefinedSplit(test_valid_fold)\n",
    "\n",
    "    clf = BernoulliNB()\n",
    "    #clf.fit(np.r_[train_data,valid_data], np.r_[train_label,valid_label])\n",
    "    clf = GridSearchCV(clf, tuned_parameters, refit=True, scoring='accuracy', cv=ps, return_train_score=True)\n",
    "    clf.fit(np.r_[train_data,valid_data], np.r_[train_label,valid_label])\n",
    "    #y_pred = clf.predict(yelp_train_x)\n",
    "\n",
    "    train_scores = clf.cv_results_['mean_train_score']\n",
    "    print('train_scores:',train_scores)\n",
    "    test_scores = clf.cv_results_['mean_test_score']\n",
    "    print('valid_scores:',test_scores)\n",
    "    params = clf.cv_results_['params']\n",
    "    print('params:', params)\n",
    "    best_param = clf.best_params_ \n",
    "    print('best_param', best_param)\n",
    "    best_estimator = clf.best_estimator_  \n",
    "    print('best_estimator', best_estimator)\n",
    "    best_score = clf.best_score_\n",
    "    print('best_score', best_score)\n",
    "\n",
    "    clf = BernoulliNB(alpha = best_param['alpha'])\n",
    "    clf.fit(train_data, train_label)\n",
    "    \n",
    "    y_pred_train = clf.predict(train_data)    \n",
    "    y_pred_valid = clf.predict(valid_data)\n",
    "    y_pred_test = clf.predict(test_data)\n",
    "    f1_train= f1_score(train_label, y_pred_train, average='micro')\n",
    "    f1_valid= f1_score(valid_label, y_pred_valid, average='micro')\n",
    "    f1_test= f1_score(test_label, y_pred_test, average='micro')\n",
    "    print('f1 (train): ', f1_train)\n",
    "    print('f1 (valid): ', f1_valid)\n",
    "    print('f1 (test): ', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.92406667 0.92293333 0.92053333 0.91553333 0.90793333 0.89      ]\n",
      "valid_scores: [0.8052 0.8135 0.8261 0.8345 0.8395 0.8393]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.1}\n",
      "best_estimator BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.8395\n",
      "f1 (train):  0.9079333333333335\n",
      "f1 (valid):  0.8395000000000001\n",
      "f1 (test):  0.80388\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes_B(train_unigram[:15000],IMDB_train_y[:15000],train_unigram[15000:],IMDB_train_y[15000:],test_unigram,IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.92706667 0.92586667 0.92333333 0.91866667 0.91033333 0.8942    ]\n",
      "valid_scores: [0.8145 0.8247 0.8335 0.8404 0.8468 0.8453]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.1}\n",
      "best_estimator BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.8468\n",
      "f1 (train):  0.9103333333333333\n",
      "f1 (valid):  0.8468\n",
      "f1 (test):  0.81612\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes_B(train_unigram_w_sw[:15000],IMDB_train_y[:15000],train_unigram_w_sw[15000:],IMDB_train_y[15000:],test_unigram_w_sw,IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.94873333 0.94846667 0.948      0.9466     0.94293333 0.92993333]\n",
      "valid_scores: [0.8051 0.8084 0.8132 0.823  0.831  0.832 ]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 1}\n",
      "best_estimator BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.832\n",
      "f1 (train):  0.9299333333333333\n",
      "f1 (valid):  0.832\n",
      "f1 (test):  0.8191999999999999\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes_B(train_bigram[:15000],IMDB_train_y[:15000],train_bigram[15000:],IMDB_train_y[15000:],test_bigram,IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.9276     0.9274     0.92713333 0.92633333 0.92473333 0.92193333]\n",
      "valid_scores: [0.8591 0.8605 0.862  0.8618 0.861  0.8609]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.001}\n",
      "best_estimator BernoulliNB(alpha=0.001, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.862\n",
      "f1 (train):  0.9271333333333334\n",
      "f1 (valid):  0.8619999999999999\n",
      "f1 (test):  0.86016\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes_B(train_bigram_w_sw[:15000],IMDB_train_y[:15000],train_bigram_w_sw[15000:],IMDB_train_y[15000:],test_bigram_w_sw,IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.90593333 0.90593333 0.90593333 0.90566667 0.90446667 0.88766667]\n",
      "valid_scores: [0.7081 0.7082 0.7083 0.7091 0.7149 0.7164]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 1}\n",
      "best_estimator BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.7164\n",
      "f1 (train):  0.8876666666666667\n",
      "f1 (valid):  0.7164\n",
      "f1 (test):  0.69028\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes_B(train_trigram[:15000],IMDB_train_y[:15000],train_trigram[15000:],IMDB_train_y[15000:],test_trigram,IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.9348     0.93466667 0.9344     0.93393333 0.93233333 0.9254    ]\n",
      "valid_scores: [0.8208 0.8239 0.8279 0.8317 0.835  0.8362]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 1}\n",
      "best_estimator BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.8362\n",
      "f1 (train):  0.9254\n",
      "f1 (valid):  0.8362\n",
      "f1 (test):  0.83884\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes_B(train_trigram_w_sw[:15000],IMDB_train_y[:15000],train_trigram_w_sw[15000:],IMDB_train_y[15000:],test_trigram_w_sw,IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Naive_Bayes(train_data, train_label, test_data, test_label, cv):\n",
    "\n",
    "    tuned_parameters = [{'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]}]\n",
    "    clf = BernoulliNB()\n",
    "    clf = GridSearchCV(clf, tuned_parameters, refit=True, scoring='accuracy', cv=cv, return_train_score=True)\n",
    "    clf.fit(train_data, train_label)\n",
    "\n",
    "    \n",
    "    train_scores = clf.cv_results_['mean_train_score']\n",
    "    print('train_scores:',train_scores)\n",
    "    test_scores = clf.cv_results_['mean_test_score']\n",
    "    print('valid_scores:',test_scores)\n",
    "    params = clf.cv_results_['params']\n",
    "    print('params:', params)\n",
    "    best_param = clf.best_params_ \n",
    "    print('best_param', best_param)\n",
    "    best_estimator = clf.best_estimator_  \n",
    "    print('best_estimator', best_estimator)\n",
    "    best_score = clf.best_score_\n",
    "    print('best_score', best_score)\n",
    "\n",
    "    clf = BernoulliNB(alpha = best_param['alpha'])\n",
    "    clf.fit(train_data, train_label)\n",
    "    \n",
    "    y_pred_train = clf.predict(train_data)    \n",
    "    y_pred_test = clf.predict(test_data)\n",
    "    f1_train= f1_score(train_label, y_pred_train, average='micro')\n",
    "    f1_test= f1_score(test_label, y_pred_test, average='micro')\n",
    "    print('f1 (train): ', f1_train)\n",
    "    print('f1 (test): ', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.9143  0.91329 0.91148 0.90837 0.90272 0.89093]\n",
      "valid_scores: [0.81928 0.8262  0.83324 0.8404  0.84524 0.84388]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.1}\n",
      "best_estimator BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.84524\n",
      "f1 (train):  0.89436\n",
      "f1 (test):  0.81292\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_unigram,IMDB_train_y,test_unigram,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.91624 0.91499 0.91327 0.90961 0.90376 0.89206]\n",
      "valid_scores: [0.8272  0.83476 0.842   0.84776 0.85156 0.8514 ]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.1}\n",
      "best_estimator BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.85156\n",
      "f1 (train):  0.89508\n",
      "f1 (test):  0.82412\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_unigram_w_sw,IMDB_train_y,test_unigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.9325  0.93231 0.93187 0.93081 0.9283  0.91866]\n",
      "valid_scores: [0.82392 0.82712 0.83108 0.83652 0.84228 0.84408]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 1}\n",
      "best_estimator BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.84408\n",
      "f1 (train):  0.91036\n",
      "f1 (test):  0.82776\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_bigram,np.asarray(IMDB_train_y),test_bigram,np.asarray(IMDB_test_y), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.91403 0.91394 0.9137  0.91334 0.9127  0.9103 ]\n",
      "valid_scores: [0.86568 0.866   0.86628 0.86648 0.86684 0.86528]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.1}\n",
      "best_estimator BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.86684\n",
      "f1 (train):  0.905\n",
      "f1 (test):  0.86616\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_bigram_w_sw,IMDB_train_y,test_bigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.89638 0.89637 0.89634 0.89544 0.89209 0.87817]\n",
      "valid_scores: [0.71588 0.71592 0.71628 0.71792 0.72204 0.72952]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 1}\n",
      "best_estimator BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.72952\n",
      "f1 (train):  0.86708\n",
      "f1 (test):  0.7022\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_trigram,np.asarray(IMDB_train_y),test_trigram,np.asarray(IMDB_test_y), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.91979 0.91971 0.91947 0.91901 0.91776 0.91342]\n",
      "valid_scores: [0.84052 0.84256 0.84444 0.8458  0.84752 0.84584]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.1}\n",
      "best_estimator BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.84752\n",
      "f1 (train):  0.9089599999999999\n",
      "f1 (test):  0.85092\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_trigram_w_sw,IMDB_train_y,test_trigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.89824 0.89815 0.8981  0.89793 0.89762 0.89592]\n",
      "valid_scores: [0.86864 0.86896 0.86908 0.86892 0.86872 0.86756]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.001}\n",
      "best_estimator BernoulliNB(alpha=0.001, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.86908\n",
      "f1 (train):  0.89244\n",
      "f1 (test):  0.86308\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_unibigram_w_sw,IMDB_train_y,test_unibigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.89585 0.89582 0.89578 0.89558 0.89525 0.89398]\n",
      "valid_scores: [0.8696  0.86956 0.8696  0.86932 0.86932 0.8678 ]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 1e-05}\n",
      "best_estimator BernoulliNB(alpha=1e-05, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.8696\n",
      "f1 (train):  0.8902\n",
      "f1 (test):  0.8664\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_allgram_w_sw,IMDB_train_y,test_allgram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.89686 0.89679 0.89667 0.89656 0.89604 0.89454]\n",
      "valid_scores: [0.86912 0.86888 0.8688  0.86856 0.86832 0.86724]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 1e-05}\n",
      "best_estimator BernoulliNB(alpha=1e-05, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.86912\n",
      "f1 (train):  0.8918\n",
      "f1 (test):  0.86688\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_allgram_wo_lm,IMDB_train_y,test_allgram_wo_lm,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.89468 0.89466 0.89459 0.89444 0.89411 0.89301]\n",
      "valid_scores: [0.86748 0.86748 0.86748 0.86732 0.86724 0.86612]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 1e-05}\n",
      "best_estimator BernoulliNB(alpha=1e-05, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.86748\n",
      "f1 (train):  0.8894\n",
      "f1 (test):  0.865\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes(train_allgram_rm_num,IMDB_train_y,test_allgram_rm_num,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import spmatrix, coo_matrix\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model.base import LinearClassifierMixin, SparseCoefMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "\n",
    "    def __init__(self, alpha=1, C=1, beta=0.25, fit_intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        self.beta = beta\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        if len(self.classes_) == 2:\n",
    "            coef_, intercept_ = self._fit_binary(X, y)\n",
    "            self.coef_ = coef_\n",
    "            self.intercept_ = intercept_\n",
    "        else:\n",
    "            coef_, intercept_ = zip(*[\n",
    "                self._fit_binary(X, y == class_)\n",
    "                for class_ in self.classes_\n",
    "            ])\n",
    "            self.coef_ = np.concatenate(coef_)\n",
    "            self.intercept_ = np.array(intercept_).flatten()\n",
    "        return self\n",
    "\n",
    "    def _fit_binary(self, X, y):\n",
    "        p = np.asarray(self.alpha + X[y == 1].sum(axis=0)).flatten()\n",
    "        q = np.asarray(self.alpha + X[y == 0].sum(axis=0)).flatten()\n",
    "        r = np.log(p/np.abs(p).sum()) - np.log(q/np.abs(q).sum())\n",
    "        b = np.log((y == 1).sum()) - np.log((y == 0).sum())\n",
    "\n",
    "        if isinstance(X, spmatrix):\n",
    "            indices = np.arange(len(r))\n",
    "            r_sparse = coo_matrix(\n",
    "                (r, (indices, indices)),\n",
    "                shape=(len(r), len(r))\n",
    "            )\n",
    "            X_scaled = X * r_sparse\n",
    "        else:\n",
    "            X_scaled = X * r\n",
    "\n",
    "        lsvc = LinearSVC(\n",
    "            C=self.C,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            max_iter=10000\n",
    "        ).fit(X_scaled, y)\n",
    "\n",
    "        mean_mag =  np.abs(lsvc.coef_).mean()\n",
    "\n",
    "        coef_ = (1 - self.beta) * mean_mag * r + self.beta * (r * lsvc.coef_)\n",
    "\n",
    "        intercept_ = (1 - self.beta) * mean_mag * b + self.beta * lsvc.intercept_\n",
    "\n",
    "        return coef_, intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB_SVM(train_data, train_label, test_data, test_label, cv):\n",
    "\n",
    "    tuned_parameters_C = [{'C': [0.01, 0.1, 1.0, 2.0]}]\n",
    "    \n",
    "    clf = NBSVM()\n",
    "    clf = GridSearchCV(clf, tuned_parameters_C, refit=True, scoring='f1_micro', cv=cv, return_train_score=True)\n",
    "    clf.fit(train_data, train_label)\n",
    "    best_param_c = clf.best_params_ \n",
    "    print('best_param', best_param_c)\n",
    "    \n",
    "    tuned_parameters_beta = [{'beta': [0.25, .5, .75]}]\n",
    "    clf = NBSVM(C=best_param_c['C'])\n",
    "    clf = GridSearchCV(clf, tuned_parameters_beta, refit=True, scoring='f1_micro', cv=cv, return_train_score=True)\n",
    "    clf.fit(train_data, train_label)\n",
    "    best_param_b = clf.best_params_ \n",
    "    print('best_param', best_param_b)\n",
    "    \n",
    "    tuned_parameters_alpha = [{'alpha': [0.001, 0.01, 0.1, 1]}]\n",
    "    clf = NBSVM(C=best_param_c['C'], beta=best_param_b['beta'])\n",
    "    clf = GridSearchCV(clf, tuned_parameters_alpha, refit=True, scoring='f1_micro', cv=cv, return_train_score=True)\n",
    "    clf.fit(train_data, train_label)\n",
    "    best_param = clf.best_params_ \n",
    "    print('best_param', best_param)\n",
    "    \n",
    "    train_scores = clf.cv_results_['mean_train_score']\n",
    "    print('train_scores:',train_scores)\n",
    "    test_scores = clf.cv_results_['mean_test_score']\n",
    "    print('valid_scores:',test_scores)\n",
    "    params = clf.cv_results_['params']\n",
    "    print('params:', params)\n",
    "    best_estimator = clf.best_estimator_  \n",
    "    print('best_estimator', best_estimator)\n",
    "    best_score = clf.best_score_\n",
    "    print('best_score', best_score)\n",
    "\n",
    "    clf = NBSVM(C=best_param_c['C'], beta=best_param_b['beta'], alpha=best_param['alpha'])\n",
    "    clf.fit(train_data, train_label)\n",
    "    \n",
    "    y_pred_train = clf.predict(train_data)    \n",
    "    y_pred_test = clf.predict(test_data)\n",
    "    f1_train= f1_score(train_label, y_pred_train, average='micro')\n",
    "    f1_test= f1_score(test_label, y_pred_test, average='micro')\n",
    "    print('f1 (train): ', f1_train)\n",
    "    print('f1 (test): ', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param {'C': 0.01}\n",
      "best_param {'beta': 0.5}\n",
      "best_param {'alpha': 1}\n",
      "train_scores: [0.96432 0.96313 0.96018 0.95212]\n",
      "valid_scores: [0.88444 0.88804 0.89212 0.89316]\n",
      "params: [{'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_estimator NBSVM(C=0.01, alpha=1, beta=0.5, fit_intercept=False)\n",
      "best_score 0.89316\n",
      "f1 (train):  0.94796\n",
      "f1 (test):  0.87652\n"
     ]
    }
   ],
   "source": [
    "NB_SVM(train_unigram,np.asarray(IMDB_train_y),test_unigram,np.asarray(IMDB_test_y), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param {'C': 0.01}\n",
      "best_param {'beta': 0.5}\n",
      "best_param {'alpha': 1}\n",
      "train_scores: [0.9651  0.96364 0.96071 0.95233]\n",
      "valid_scores: [0.88916 0.89296 0.89616 0.89784]\n",
      "params: [{'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_estimator NBSVM(C=0.01, alpha=1, beta=0.5, fit_intercept=False)\n",
      "best_score 0.89784\n",
      "f1 (train):  0.94912\n",
      "f1 (test):  0.8836\n"
     ]
    }
   ],
   "source": [
    "NB_SVM(train_unigram_w_sw,IMDB_train_y,test_unigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param {'C': 0.01}\n",
      "best_param {'beta': 0.5}\n",
      "best_param {'alpha': 1}\n",
      "train_scores: [0.96277 0.96262 0.96223 0.95953]\n",
      "valid_scores: [0.89136 0.89156 0.89196 0.89228]\n",
      "params: [{'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_estimator NBSVM(C=0.01, alpha=1, beta=0.5, fit_intercept=False)\n",
      "best_score 0.89228\n",
      "f1 (train):  0.95484\n",
      "f1 (test):  0.8904800000000002\n"
     ]
    }
   ],
   "source": [
    "NB_SVM(train_bigram_w_sw,IMDB_train_y,test_bigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param {'C': 0.01}\n",
      "best_param {'beta': 0.25}\n",
      "best_param {'alpha': 0.1}\n",
      "train_scores: [0.94452 0.94405 0.94293 0.93719]\n",
      "valid_scores: [0.8518  0.85408 0.85596 0.85536]\n",
      "params: [{'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_estimator NBSVM(C=0.01, alpha=0.1, beta=0.25, fit_intercept=False)\n",
      "best_score 0.85596\n",
      "f1 (train):  0.9326\n",
      "f1 (test):  0.85784\n"
     ]
    }
   ],
   "source": [
    "NB_SVM(train_trigram_w_sw,IMDB_train_y,test_trigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param {'C': 0.01}\n",
      "best_param {'beta': 0.75}\n",
      "best_param {'alpha': 0.1}\n",
      "train_scores: [0.97696 0.97692 0.97661 0.97479]\n",
      "valid_scores: [0.90648 0.907   0.90704 0.90672]\n",
      "params: [{'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_estimator NBSVM(C=0.01, alpha=0.1, beta=0.75, fit_intercept=False)\n",
      "best_score 0.90704\n",
      "f1 (train):  0.97248\n",
      "f1 (test):  0.90344\n"
     ]
    }
   ],
   "source": [
    "NB_SVM(train_unibigram_w_sw,IMDB_train_y,test_unibigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param {'C': 0.01}\n",
      "best_param {'beta': 0.75}\n",
      "best_param {'alpha': 0.1}\n",
      "train_scores: [0.97608 0.97603 0.9759  0.97439]\n",
      "valid_scores: [0.90548 0.90564 0.90588 0.9058 ]\n",
      "params: [{'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_estimator NBSVM(C=0.01, alpha=0.1, beta=0.75, fit_intercept=False)\n",
      "best_score 0.90588\n",
      "f1 (train):  0.97084\n",
      "f1 (test):  0.90476\n"
     ]
    }
   ],
   "source": [
    "NB_SVM(train_allgram_w_sw,IMDB_train_y,test_allgram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
